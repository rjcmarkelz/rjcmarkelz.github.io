{
  "hash": "7617a51c15ac3e6348640aa21ae2e275",
  "result": {
    "markdown": "---\ntitle: Scraping Weather Data 2\nauthor: RJ Cody Markelz\ndescription: --Avalanche Website Data Scraping\ndate: 2023-09-22\ncategories: [weather, data, scraping, website, meta-data, research, journalism, data journalism, avalanche, ski touring, back country skiing]\nimage: /images/data-scraping-website.png\n---\n\n\n\n\nThis post is the second in a series teaching data scraping and visualization techniques to data journalists. See the other posts in the series [Post 1](/posts/2023-09-21-scraping-weather-data-setup.html), [Post 3](/posts/2023-09-23-summarizing-weather-data.html) and [Post 4](/posts/2023-10-01-avalanche-data-animation.html).\n\n\n### Introduction\nI started collaborating with the [Mount Shasta Avalanche Center](https://www.shastaavalanche.org/#/all) for a long form data journalism project looking at snow and avalanche condition forecasting with the backdrop of climate change. This adds an additional layer of uncertainty into any type of short-term forecast. Forecasters put out daily forecasts that integrate a lot of weather, snowfall, wind speed, direction, terrain, and previous snowfall information along with on the ground observational data collected from snow pits. [Here](https://www.shastaavalanche.org/page/how-read-advisory) is a brief summary of how to read a forecast.\n\nI thought this would be a good opportunity to show how you can collect, clean, and visualize your own data-sets for data journalism projects. I will be scraping the Avalanche Center's public website to assemble an aggregated data-set of my own to ask my own questions. This is a series of posts on the topic using open-source data tools. \n\nThis is a post showing how to extract data from a website and make a few plots. I chose the [Mount Shasta Avalanche Center](https://www.shastaavalanche.org/page/seasonal-weather-history-mount-shasta) data because I monitor this everyday throughout the season to see how the avalanche forecast changes and how the snowpack is developing. I did an intro post on this topic last year, but I would like to go into more depth on extracting information from a website. \n\nThere is a great website scraping package that is part of the tidyverse called Rvest. Check out the [Documentation](https://rvest.tidyverse.org/). The avalanche center website has a number of selectors on it to choose which range of data you would like displayed. We will be using the Selenium package in order to be able to do that and accessing it from R via the RSelenium package. Selenium runs a minimal version of a web browser that can interact with webpages. So instead of point and clicking, we can programatically interact with the website using Selenium. I will write another post on how to set that up later, but for now, load the libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RSelenium)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(lubridate)\n```\n:::\n\n\nI have started the docker container that has selenium running inside. Now with my R session I will connect to that container and open the connection to the remote driver section. Please see the [last post](/posts/2023-09-21-scraping-weather-data-setup.html) in this series for how to set this up.\n\n::: {.cell}\n\n```{.r .cell-code}\nremDr <- remoteDriver(\n  remoteServerAddr = \"selenium-container\",\n  port = 4444L,\n  browserName = \"firefox\",\n  version = \"78.0\"  # e.g., \"91.0\"\n)\n\nremDr$open()\n```\n:::\n\n\n\nLoad the URL that we are going to be interacting with to extract the information.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Navigate to the website\nurl <- \"https://www.shastaavalanche.org/page/seasonal-weather-history-mount-shasta\"\nremDr$navigate(url)\n```\n:::\n\n\n\nPoint your regular web browser to \"https://www.shastaavalanche.org/page/seasonal-weather-history-mount-shasta\" and right click anywhere on the page. Your web browser will likely have an \"Inspect\" option what will pull up split screen view of the webpage. The top will have the regular webpage you were viewing. The bottom will have an element view of the website. You can click around on the elements and find the names of elements that you want interact with programatically. \n\n\nNow we can programmatically select the elements on the page. I am selecting October 1, 2017 as the start of the date range and April 30, 2023 as the end of the date range. We then submit the query by clicking on that button and build in a sleep timer so that the page has time to load inside our Selenium session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonth_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='start_month']\")\nmonth_dropdown$clickElement()\n\nselected_month <- remDr$findElement(using = \"css selector\", \"select[name='start_month'] option[value='Oct']\")\nselected_month$clickElement()\n\nyear_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='start_year']\")\nyear_dropdown$clickElement()\n\nselected_year <- remDr$findElement(using = \"css selector\", \"select[name='start_year'] option[value='2017']\")\nselected_year$clickElement()\n\nday_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='start_day']\")\nday_dropdown$clickElement()\n\nselected_day <- remDr$findElement(using = \"css selector\", \"select[name='start_day'] option[value='1']\")\nselected_day$clickElement()\n\nend_month_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='end_month']\")\nend_month_dropdown$clickElement()\n\nselected_end_month <- remDr$findElement(using = \"css selector\", \"select[name='end_month'] option[value='Apr']\")\nselected_end_month$clickElement()\n\nend_year_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='end_year']\")\nend_year_dropdown$clickElement()\n\nselected_end_year <- remDr$findElement(using = \"css selector\", \"select[name='end_year'] option[value='2023']\")\nselected_end_year$clickElement()\n\nend_day_dropdown <- remDr$findElement(using = \"css selector\", \"select[name='end_day']\")\nend_day_dropdown$clickElement()\n\nselected_end_day <- remDr$findElement(using = \"css selector\", \"select[name='end_day'] option[value='30']\")\nselected_end_day$clickElement()\n\nsubmit_button <- remDr$findElement(using = \"css selector\", \"button[title='Submit Query']\")\nsubmit_button$clickElement()\n\nSys.sleep(30)\n```\n:::\n\n\nOnce the page is loaded inside Selenium we can read the page into R saving it as **parsed_content**. We then select the weather history table and extract all of those features.\n\n::: {.cell}\n\n```{.r .cell-code}\n# make sure rvest is loaded\npage_source <- remDr$getPageSource()[[1]]\n\nparsed_content <- read_html(page_source)\n\n# right click on the page to see the tabl\n\nparsed_content %>%\n    html_element(\".msac-wx-history-table\") %>%\n    html_table()\n```\n:::\n\n\nRight click on the page in your web browser and get the xpath to a specific table.\n\n::: {.cell}\n\n```{.r .cell-code}\nxpath <- \"/html/body/div[2]/main/div/article/div/table[2]\"\nweather <- html_nodes(parsed_content, xpath = xpath)\nhtml_table(weather)\n```\n:::\n\n\nFinally we will make a data frame with the weather data and clean it up using R functions.\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a data.frame with the table\nweather2 <- as.data.frame(html_table(weather, fill=TRUE))\n\n# rename columns\nnames(weather2) <- paste(weather2[1,], weather2[2,])\nnames(weather2)\nnames(weather2)[1] <- paste(\"date\")\n\n# remove rows that are now column names\nweather2 <- weather2[-c(1,2),]\n\n# take a look\nglimpse(weather2)\n\n# columns that are numeric should be converted back to such. They were coerced into character vectors because of the first two rows were characters.\nweather2 <- weather2 %>%\nmutate_at(c(2:8), as.numeric)\n\nweather2 <- weather2 %>%\nmutate_at(c(10:20), as.numeric)\n\n# coerce date column\nweather2 <- weather2 %>%\nmutate_at(1, as_date)\n\n# take a quick look\nhead(weather2)\nglimpse(weather2)\nunique(weather2$`Fx Rating `)\n# [1] \"LOW\"       \"MOD\"       \"CON\"       \"Fx Rating\" \"\"          \"HIGH\" \n\n# remove the rows that are blank or have Fx Rating - these are table formatting errors from the html\nrows_drop <- c(\"Fx Rating\", \"\")\nweather3 <- weather2[!(weather2$`Fx Rating ` %in% rows_drop), ]\n\n# Close the session\nremDr$close()\n```\n:::\n\n\n\n\nFinally we will saved the scrapped data to an .RData file so you can access it without rerunning the code above.\n\n::: {.cell}\n\n```{.r .cell-code}\nsave(weather3, file = \"~/DATA/data/Avalanche-Data-2017-2023.RData\")\n```\n:::\n\n\nSee the other posts in the series [Post 1](/posts/2023-09-21-scraping-weather-data-setup.html), [Post 3](/posts/2023-09-23-summarizing-weather-data.html) and [Post 4](/posts/2023-10-01-avalanche-data-animation.html).\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}